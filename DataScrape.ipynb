{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPr5t+NJ990c4lW6e3hEoxX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8Q-a9jutErP"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import time\n",
        "from tqdm import tqdm, trange\n",
        "from openpyxl import Workbook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.lamaistas.lt/visi-receptai/\"\n",
        "urls_file = \"urls.txt\"\n",
        "output_data = \"data.xlsx\""
      ],
      "metadata": {
        "id": "2TLWY8eNxXKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = round(25419/44)\n",
        "def ScrapeLinks(url, output_file):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "    for i in tqdm(range(1, n+1), desc=\"Scraping pages\"):\n",
        "        new_url = f\"{url}{i}\"\n",
        "        try:\n",
        "            response = requests.get(new_url, headers=headers)\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            for a in soup.find_all(\"a\", href=True):\n",
        "                if \"/receptas/\" in a[\"href\"]:\n",
        "                    full_link = a[\"href\"]\n",
        "                    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
        "                        f.write(full_link + \"\\n\")\n",
        "            time.sleep(0.3)\n",
        "        except:\n",
        "            print(f\"Error in page {i}\")\n",
        "            continue\n",
        "\n",
        "    #Delete duplicates\n",
        "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "        unique_lines = list(set(lines))\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(unique_lines)"
      ],
      "metadata": {
        "id": "66DbRqWvxedh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ScrapeLinks(url, urls_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g2zftHR3ngV",
        "outputId": "10843b88-23be-43ae-9e80-2e8e6899738a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping pages: 100%|██████████| 578/578 [07:26<00:00,  1.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "def clean(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    return text.replace(\"\\n\", \" \").replace(\"\\r\", \"\").strip()\n",
        "\n",
        "def scrape_single_url(url, headers):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Title\n",
        "        title_tag = soup.find(\"div\", class_=\"recipeTitleSegment\")\n",
        "        title = title_tag.text.strip() if title_tag else \"\"\n",
        "\n",
        "        # Author description\n",
        "        authors_tag = soup.find(\"span\", class_=\"authorsDescription full\")\n",
        "        authors_desc = authors_tag.text.strip() if authors_tag else \"\"\n",
        "\n",
        "        # Portions\n",
        "        portions_tag = soup.find(class_=\"portionContainer\")\n",
        "        portions = portions_tag.text.strip().replace(\"\\n\", \"\") if portions_tag else \"\"\n",
        "\n",
        "        # Preparation time\n",
        "        prep_tag = soup.find(\"span\", class_=\"info\")\n",
        "        prep_time = prep_tag.text.strip() if prep_tag else \"\"\n",
        "\n",
        "        # Ingredients\n",
        "        amount_list = soup.find_all(class_=\"amount\")\n",
        "        amount_text = [a.text.strip() for a in amount_list]\n",
        "\n",
        "        ingredient_list = soup.find_all(\"span\", class_=\"ingredient\")\n",
        "        ingredient = [i.text.strip() for i in ingredient_list]\n",
        "        ingredient_list = [i for i in ingredient if i]\n",
        "\n",
        "        ingredients = []\n",
        "        for j in range(min(len(amount_text), len(ingredient_list))):\n",
        "            ingredients.append(amount_text[j] + \" \" + ingredient_list[j])\n",
        "        ingredients = \"; \".join(ingredients)\n",
        "\n",
        "        # Steps\n",
        "        steps_list = soup.find_all(\"div\", class_=\"description\")\n",
        "        steps_list = [s.find(\"div\", class_=\"text\") for s in steps_list]\n",
        "        steps = [s.text.strip() for s in steps_list if s]\n",
        "        steps = \"; \".join(steps)\n",
        "\n",
        "        # Clean\n",
        "        title = clean(title)\n",
        "        authors_desc = clean(authors_desc)\n",
        "        portions = clean(portions)\n",
        "        prep_time = clean(prep_time)\n",
        "        ingredients = clean(ingredients)\n",
        "        steps = clean(steps)\n",
        "\n",
        "        return [title, authors_desc, portions, prep_time, ingredients, steps]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in url {url.strip()}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def ScrapeData(urls_file, output_xlsx):\n",
        "    with open(urls_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        urls = [u.strip() for u in f.readlines() if u.strip()]\n",
        "\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "    wb = Workbook()\n",
        "    ws = wb.active\n",
        "    ws.append([\"title\", \"authors_desc\", \"portions\", \"prep_time\", \"ingredients\", \"steps\"])\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
        "        future_to_url = {executor.submit(scrape_single_url, url, headers): url for url in urls}\n",
        "        for future in tqdm(as_completed(future_to_url), total=len(urls), desc=\"Scraping data\"):\n",
        "            result = future.result()\n",
        "            if result:\n",
        "                ws.append(result)\n",
        "\n",
        "    wb.save(output_xlsx)"
      ],
      "metadata": {
        "id": "tAUYLRVWroaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ScrapeData(urls_file, output_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UiBmAA9sdA0",
        "outputId": "c224f994-70f0-4019-8b1d-d5421e297b9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping data:  19%|█▉        | 4888/25419 [09:12<52:12,  6.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in url https://www.lamaistas.lt/receptas/mesos-pyragas-su-idaru-30495: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping data: 100%|██████████| 25419/25419 [51:50<00:00,  8.17it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(output_data)"
      ],
      "metadata": {
        "id": "P3ojw9n8_18c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ec8c749e-b8aa-4cb4-8a9c-424659eb770d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_22adaa6c-65ee-4c00-bf4a-0e081002647a\", \"data.xlsx\", 10786028)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}