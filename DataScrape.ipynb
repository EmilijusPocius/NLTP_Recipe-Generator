{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMM5Se41Ux4Q695cfI08eXb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "K8Q-a9jutErP"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import time\n",
        "from tqdm import tqdm, trange\n",
        "from openpyxl import Workbook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.lamaistas.lt/visi-receptai/\"\n",
        "urls_file = \"urls.txt\"\n",
        "output_csv = \"data.csv\""
      ],
      "metadata": {
        "id": "2TLWY8eNxXKj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = round(25419/44)\n",
        "def ScrapeLinks(url, output_file):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "    for i in tqdm(range(1, n+1), desc=\"Scraping pages\"):\n",
        "        new_url = f\"{url}{i}\"\n",
        "        try:\n",
        "            response = requests.get(new_url, headers=headers)\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            for a in soup.find_all(\"a\", href=True):\n",
        "                if \"/receptas/\" in a[\"href\"]:\n",
        "                    full_link = a[\"href\"]\n",
        "                    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
        "                        f.write(full_link + \"\\n\")\n",
        "            time.sleep(0.3)\n",
        "        except:\n",
        "            print(f\"Error in page {i}\")\n",
        "            continue\n",
        "\n",
        "    #Delete duplicates\n",
        "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "        unique_lines = list(set(lines))\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(unique_lines)"
      ],
      "metadata": {
        "id": "66DbRqWvxedh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ScrapeLinks(url, urls_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g2zftHR3ngV",
        "outputId": "10843b88-23be-43ae-9e80-2e8e6899738a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping pages: 100%|██████████| 578/578 [07:26<00:00,  1.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    return text.replace(\"\\n\", \" \").replace(\"\\r\", \"\").strip()\n",
        "\n",
        "def ScrapeData(urls_file, output_xlsx):\n",
        "    with open(urls_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        urls = [u.strip() for u in f.readlines() if u.strip()]\n",
        "\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "    wb = Workbook()\n",
        "    ws = wb.active\n",
        "    ws.append([\"title\", \"authors_desc\", \"portions\", \"prep_time\", \"ingredients\", \"steps\"])\n",
        "\n",
        "    for url in tqdm(urls, desc=\"Scraping data\"):\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            # Title\n",
        "            title_tag = soup.find(\"div\", class_=\"recipeTitleSegment\")\n",
        "            title = title_tag.text.strip() if title_tag else \"\"\n",
        "\n",
        "            # Author description\n",
        "            authors_tag = soup.find(\"span\", class_=\"authorsDescription full\")\n",
        "            authors_desc = authors_tag.text.strip() if authors_tag else \"\"\n",
        "\n",
        "            # Portions\n",
        "            portions_tag = soup.find(class_=\"portionContainer\")\n",
        "            portions = portions_tag.text.strip().replace(\"\\n\", \"\") if portions_tag else \"\"\n",
        "\n",
        "            # Preparation time\n",
        "            prep_tag = soup.find(\"span\", class_=\"info\")\n",
        "            prep_time = prep_tag.text.strip() if prep_tag else \"\"\n",
        "\n",
        "            # Ingredients\n",
        "            amount_list = soup.find_all(class_=\"amount\")\n",
        "            amount_text = [a.text.strip() for a in amount_list]\n",
        "\n",
        "            ingredient_list = soup.find_all(class_=\"ingredient\")\n",
        "            ingredient_ids = [ing.get(\"id\") for ing in ingredient_list if ing.get(\"id\")]\n",
        "\n",
        "            ingredients = []\n",
        "            for j in range(min(len(amount_text), len(ingredient_ids))):\n",
        "                ingredients.append(amount_text[j] + \" \" + ingredient_ids[j])\n",
        "            ingredients = \"; \".join(ingredients)\n",
        "\n",
        "            # Steps\n",
        "            steps_list = soup.find_all(\"div\", class_=\"description\")\n",
        "            steps_list = [s.find(\"div\", class_=\"text\") for s in steps_list]\n",
        "            steps = [s.text.strip() for s in steps_list if s]\n",
        "            steps = \"; \".join(steps)\n",
        "\n",
        "            # Clean\n",
        "            title = clean(title)\n",
        "            authors_desc = clean(authors_desc)\n",
        "            portions = clean(portions)\n",
        "            prep_time = clean(prep_time)\n",
        "            ingredients = clean(ingredients)\n",
        "            steps = clean(steps)\n",
        "\n",
        "            ws.append([title, authors_desc, portions, prep_time, ingredients, steps])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in url {url.strip()}: {e}\")\n",
        "            continue\n",
        "\n",
        "    wb.save(output_xlsx)\n"
      ],
      "metadata": {
        "id": "tAUYLRVWroaH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ScrapeData(urls_file, output_csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UiBmAA9sdA0",
        "outputId": "211a91ed-1509-4ea6-a5d6-93dc260a4066"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping data:  27%|██▋       | 6761/25419 [1:24:09<2:49:49,  1.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in url https://www.lamaistas.lt/receptas/pomidoru-ir-obuoliu-marmeladas-su-aitriaja-paprika-60359: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping data:  92%|█████████▏| 23392/25419 [4:47:09<20:37,  1.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in url https://www.lamaistas.lt/receptas/nekeptas-gaivus-varskes-tortas-37628: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping data: 100%|██████████| 25419/25419 [5:12:42<00:00,  1.35it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('data.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "P3ojw9n8_18c",
        "outputId": "2a39ff82-cd18-4863-c1d9-546ec8e47a8f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9cf7256c-af52-4956-a0d2-759c66d24a2c\", \"data.csv\", 10055674)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}